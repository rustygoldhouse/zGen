{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention_Decon.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rustygoldhouse/zGen/blob/master/Attention_Decon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmKuUusht6xq",
        "colab_type": "code",
        "outputId": "17b61a04-3ad2-45ff-c4b6-85270f7f9512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%content/drive/My\\ Drive/Zairja\n",
        "!git clone https://github.com/slme1109/Text_Generation_with_Attention_Mechanism.git\n",
        "%cd content/drive/My\\ Drive/Zairja/Text_Generation_with_Attention_Mechanism"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "[Errno 2] No such file or directory: 'content/drive/My Drive/Zairja'\n",
            "/content\n",
            "Cloning into 'Text_Generation_with_Attention_Mechanism'...\n",
            "remote: Enumerating objects: 224, done.\u001b[K\n",
            "remote: Counting objects: 100% (224/224), done.\u001b[K\n",
            "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
            "remote: Total 224 (delta 68), reused 188 (delta 36), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (224/224), 85.56 MiB | 35.40 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n",
            "[Errno 2] No such file or directory: 'content/drive/My Drive/Zairja/Text_Generation_with_Attention_Mechanism'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZBRGXHMtu5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text=open('/content/drive/My Drive/Zairja/data/z_short_md.txt').read()\n",
        "# text=[i for i in text if i]\n",
        "# text='\\n'.join(text)\n",
        "# # text[:50]\n",
        "print(text, file=open('data/pop/drake.txt', 'w'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVhfe3_bvxjM",
        "colab_type": "code",
        "outputId": "1208d22c-c4a0-451c-f60a-ff4bc9490dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# %cd src\n",
        "!python train.py"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<====================| Data Cleaning |====================>\n",
            "<===| Read 24543 sentence pairs from Doc 1 | Trimmed to 23615 sentence pairs | Counted 16886 unique words in total|===>\n",
            "\n",
            "<====================| Dataset Summary |====================>\n",
            "<===| Loaded 23615 sentence pairs | 16886 unique words |===>\n",
            "\n",
            "<====================| Training Summary |====================>\n",
            "<===| Epochs:30 | BatchSize:100 | ClipValue:5.0 | EncoderLR:1.0e-03 | DecoderLR:3.0e-03 | ValidationSize:0.30 |===>\n",
            "\n",
            "<===| Training on epoch:1/30 batch:1/236... | time used 0m:0s left 0m:1s |===>\n",
            "<===| Training on epoch:1/30 batch:2/236... | time used 0m:12s left 23m:38s |===>\n",
            "<===| Training on epoch:1/30 batch:3/236... | time used 0m:23s left 31m:2s |===>\n",
            "<===| Training on epoch:1/30 batch:4/236... | time used 0m:35s left 34m:41s |===>\n",
            "<===| Training on epoch:1/30 batch:5/236... | time used 0m:47s left 36m:47s |===>\n",
            "<===| Training on epoch:1/30 batch:6/236... | time used 0m:59s left 38m:13s |===>\n",
            "<===| Training on epoch:1/30 batch:7/236... | time used 1m:11s left 39m:11s |===>\n",
            "<===| Training on epoch:1/30 batch:8/236... | time used 1m:23s left 39m:47s |===>\n",
            "<===| Training on epoch:1/30 batch:9/236... | time used 1m:35s left 40m:12s |===>\n",
            "<===| Training on epoch:1/30 batch:10/236... | time used 1m:47s left 40m:31s |===>\n",
            "<===| Training on epoch:1/30 batch:11/236... | time used 1m:59s left 40m:44s |===>\n",
            "<===| Training on epoch:1/30 batch:12/236... | time used 2m:11s left 40m:57s |===>\n",
            "<===| Training on epoch:1/30 batch:13/236... | time used 2m:23s left 41m:2s |===>\n",
            "<===| Training on epoch:1/30 batch:14/236... | time used 2m:35s left 41m:5s |===>\n",
            "<===| Training on epoch:1/30 batch:15/236... | time used 2m:47s left 41m:6s |===>\n",
            "<===| Training on epoch:1/30 batch:16/236... | time used 2m:58s left 40m:54s |===>\n",
            "<===| Training on epoch:1/30 batch:17/236... | time used 3m:10s left 40m:56s |===>\n",
            "<===| Training on epoch:1/30 batch:18/236... | time used 3m:22s left 40m:54s |===>\n",
            "<===| Training on epoch:1/30 batch:19/236... | time used 3m:34s left 40m:51s |===>\n",
            "<===| Training on epoch:1/30 batch:20/236... | time used 3m:45s left 40m:38s |===>\n",
            "<===| Training on epoch:1/30 batch:21/236... | time used 3m:57s left 40m:34s |===>\n",
            "<===| Training on epoch:1/30 batch:22/236... | time used 4m:9s left 40m:31s |===>\n",
            "<===| Training on epoch:1/30 batch:23/236... | time used 4m:21s left 40m:24s |===>\n",
            "<===| Training on epoch:1/30 batch:24/236... | time used 4m:33s left 40m:17s |===>\n",
            "<===| Training on epoch:1/30 batch:25/236... | time used 4m:45s left 40m:10s |===>\n",
            "<===| Training on epoch:1/30 batch:26/236... | time used 4m:57s left 40m:3s |===>\n",
            "<===| Training on epoch:1/30 batch:27/236... | time used 5m:9s left 39m:57s |===>\n",
            "<===| Training on epoch:1/30 batch:28/236... | time used 5m:21s left 39m:49s |===>\n",
            "<===| Training on epoch:1/30 batch:29/236... | time used 5m:33s left 39m:41s |===>\n",
            "<===| Training on epoch:1/30 batch:30/236... | time used 5m:45s left 39m:32s |===>\n",
            "<===| Training on epoch:1/30 batch:31/236... | time used 5m:57s left 39m:24s |===>\n",
            "<===| Training on epoch:1/30 batch:32/236... | time used 6m:8s left 39m:11s |===>\n",
            "<===| Training on epoch:1/30 batch:33/236... | time used 6m:20s left 39m:2s |===>\n",
            "<===| Training on epoch:1/30 batch:34/236... | time used 6m:32s left 38m:52s |===>\n",
            "<===| Training on epoch:1/30 batch:35/236... | time used 6m:44s left 38m:43s |===>\n",
            "<===| Training on epoch:1/30 batch:36/236... | time used 6m:56s left 38m:34s |===>\n",
            "<===| Training on epoch:1/30 batch:37/236... | time used 7m:8s left 38m:25s |===>\n",
            "<===| Training on epoch:1/30 batch:38/236... | time used 7m:20s left 38m:15s |===>\n",
            "<===| Training on epoch:1/30 batch:39/236... | time used 7m:32s left 38m:5s |===>\n",
            "<===| Training on epoch:1/30 batch:40/236... | time used 7m:44s left 37m:56s |===>\n",
            "<===| Training on epoch:1/30 batch:41/236... | time used 7m:55s left 37m:42s |===>\n",
            "<===| Training on epoch:1/30 batch:42/236... | time used 8m:7s left 37m:32s |===>\n",
            "<===| Training on epoch:1/30 batch:43/236... | time used 8m:19s left 37m:22s |===>\n",
            "<===| Training on epoch:1/30 batch:44/236... | time used 8m:30s left 37m:8s |===>\n",
            "<===| Training on epoch:1/30 batch:45/236... | time used 8m:42s left 36m:58s |===>\n",
            "<===| Training on epoch:1/30 batch:46/236... | time used 8m:54s left 36m:47s |===>\n",
            "<===| Training on epoch:1/30 batch:47/236... | time used 9m:6s left 36m:37s |===>\n",
            "<===| Training on epoch:1/30 batch:48/236... | time used 9m:18s left 36m:27s |===>\n",
            "<===| Training on epoch:1/30 batch:49/236... | time used 9m:30s left 36m:16s |===>\n",
            "<===| Training on epoch:1/30 batch:50/236... | time used 9m:42s left 36m:6s |===>\n",
            "<===| Training on epoch:1/30 batch:51/236... | time used 9m:54s left 35m:55s |===>\n",
            "<===| Training on epoch:1/30 batch:52/236... | time used 10m:6s left 35m:45s |===>\n",
            "<===| Training on epoch:1/30 batch:53/236... | time used 10m:18s left 35m:34s |===>\n",
            "<===| Training on epoch:1/30 batch:54/236... | time used 10m:30s left 35m:23s |===>\n",
            "<===| Training on epoch:1/30 batch:55/236... | time used 10m:41s left 35m:10s |===>\n",
            "<===| Training on epoch:1/30 batch:56/236... | time used 10m:53s left 34m:59s |===>\n",
            "<===| Training on epoch:1/30 batch:57/236... | time used 11m:5s left 34m:49s |===>\n",
            "<===| Training on epoch:1/30 batch:58/236... | time used 11m:17s left 34m:38s |===>\n",
            "<===| Training on epoch:1/30 batch:59/236... | time used 11m:29s left 34m:27s |===>\n",
            "<===| Training on epoch:1/30 batch:60/236... | time used 11m:41s left 34m:16s |===>\n",
            "<===| Training on epoch:1/30 batch:61/236... | time used 11m:53s left 34m:5s |===>\n",
            "<===| Training on epoch:1/30 batch:62/236... | time used 12m:5s left 33m:55s |===>\n",
            "<===| Training on epoch:1/30 batch:63/236... | time used 12m:17s left 33m:44s |===>\n",
            "<===| Training on epoch:1/30 batch:64/236... | time used 12m:29s left 33m:32s |===>\n",
            "<===| Training on epoch:1/30 batch:65/236... | time used 12m:40s left 33m:21s |===>\n",
            "<===| Training on epoch:1/30 batch:66/236... | time used 12m:52s left 33m:10s |===>\n",
            "<===| Training on epoch:1/30 batch:67/236... | time used 13m:4s left 32m:59s |===>\n",
            "<===| Training on epoch:1/30 batch:68/236... | time used 13m:16s left 32m:48s |===>\n",
            "<===| Training on epoch:1/30 batch:69/236... | time used 13m:28s left 32m:37s |===>\n",
            "<===| Training on epoch:1/30 batch:70/236... | time used 13m:40s left 32m:26s |===>\n",
            "<===| Training on epoch:1/30 batch:71/236... | time used 13m:52s left 32m:15s |===>\n",
            "<===| Training on epoch:1/30 batch:72/236... | time used 14m:4s left 32m:3s |===>\n",
            "<===| Training on epoch:1/30 batch:73/236... | time used 14m:16s left 31m:52s |===>\n",
            "<===| Training on epoch:1/30 batch:74/236... | time used 14m:28s left 31m:41s |===>\n",
            "<===| Training on epoch:1/30 batch:75/236... | time used 14m:40s left 31m:29s |===>\n",
            "<===| Training on epoch:1/30 batch:76/236... | time used 14m:52s left 31m:18s |===>\n",
            "<===| Training on epoch:1/30 batch:77/236... | time used 15m:4s left 31m:7s |===>\n",
            "<===| Training on epoch:1/30 batch:78/236... | time used 15m:16s left 30m:55s |===>\n",
            "<===| Training on epoch:1/30 batch:79/236... | time used 15m:28s left 30m:44s |===>\n",
            "<===| Training on epoch:1/30 batch:80/236... | time used 15m:40s left 30m:33s |===>\n",
            "<===| Training on epoch:1/30 batch:81/236... | time used 15m:52s left 30m:21s |===>\n",
            "<===| Training on epoch:1/30 batch:82/236... | time used 16m:3s left 30m:10s |===>\n",
            "<===| Training on epoch:1/30 batch:83/236... | time used 16m:15s left 29m:59s |===>\n",
            "<===| Training on epoch:1/30 batch:84/236... | time used 16m:27s left 29m:47s |===>\n",
            "<===| Training on epoch:1/30 batch:85/236... | time used 16m:39s left 29m:36s |===>\n",
            "<===| Training on epoch:1/30 batch:86/236... | time used 16m:51s left 29m:24s |===>\n",
            "<===| Training on epoch:1/30 batch:87/236... | time used 17m:3s left 29m:13s |===>\n",
            "<===| Training on epoch:1/30 batch:88/236... | time used 17m:15s left 29m:1s |===>\n",
            "<===| Training on epoch:1/30 batch:89/236... | time used 17m:27s left 28m:50s |===>\n",
            "<===| Training on epoch:1/30 batch:90/236... | time used 17m:38s left 28m:37s |===>\n",
            "<===| Training on epoch:1/30 batch:91/236... | time used 17m:50s left 28m:26s |===>\n",
            "<===| Training on epoch:1/30 batch:92/236... | time used 18m:2s left 28m:14s |===>\n",
            "<===| Training on epoch:1/30 batch:93/236... | time used 18m:14s left 28m:3s |===>\n",
            "<===| Training on epoch:1/30 batch:94/236... | time used 18m:25s left 27m:50s |===>\n",
            "<===| Training on epoch:1/30 batch:95/236... | time used 18m:37s left 27m:38s |===>\n",
            "<===| Training on epoch:1/30 batch:96/236... | time used 18m:49s left 27m:27s |===>\n",
            "<===| Training on epoch:1/30 batch:97/236... | time used 19m:1s left 27m:15s |===>\n",
            "<===| Training on epoch:1/30 batch:98/236... | time used 19m:13s left 27m:4s |===>\n",
            "<===| Training on epoch:1/30 batch:99/236... | time used 19m:25s left 26m:52s |===>\n",
            "<===| Training on epoch:1/30 batch:100/236... | time used 19m:37s left 26m:41s |===>\n",
            "<===| Training on epoch:1/30 batch:101/236... | time used 19m:49s left 26m:29s |===>\n",
            "<===| Training on epoch:1/30 batch:102/236... | time used 20m:1s left 26m:18s |===>\n",
            "<===| Training on epoch:1/30 batch:103/236... | time used 20m:13s left 26m:6s |===>\n",
            "<===| Training on epoch:1/30 batch:104/236... | time used 20m:25s left 25m:54s |===>\n",
            "<===| Training on epoch:1/30 batch:105/236... | time used 20m:37s left 25m:43s |===>\n",
            "<===| Training on epoch:1/30 batch:106/236... | time used 20m:48s left 25m:31s |===>\n",
            "<===| Training on epoch:1/30 batch:107/236... | time used 21m:0s left 25m:20s |===>\n",
            "<===| Training on epoch:1/30 batch:108/236... | time used 21m:12s left 25m:8s |===>\n",
            "<===| Training on epoch:1/30 batch:109/236... | time used 21m:24s left 24m:57s |===>\n",
            "<===| Training on epoch:1/30 batch:110/236... | time used 21m:36s left 24m:45s |===>\n",
            "<===| Training on epoch:1/30 batch:111/236... | time used 21m:48s left 24m:33s |===>\n",
            "<===| Training on epoch:1/30 batch:112/236... | time used 22m:0s left 24m:22s |===>\n",
            "<===| Training on epoch:1/30 batch:113/236... | time used 22m:12s left 24m:10s |===>\n",
            "<===| Training on epoch:1/30 batch:114/236... | time used 22m:24s left 23m:58s |===>\n",
            "<===| Training on epoch:1/30 batch:115/236... | time used 22m:36s left 23m:47s |===>\n",
            "<===| Training on epoch:1/30 batch:116/236... | time used 22m:48s left 23m:35s |===>\n",
            "<===| Training on epoch:1/30 batch:117/236... | time used 23m:0s left 23m:23s |===>\n",
            "<===| Training on epoch:1/30 batch:118/236... | time used 23m:12s left 23m:12s |===>\n",
            "<===| Training on epoch:1/30 batch:119/236... | time used 23m:24s left 23m:0s |===>\n",
            "<===| Training on epoch:1/30 batch:120/236... | time used 23m:36s left 22m:48s |===>\n",
            "<===| Training on epoch:1/30 batch:121/236... | time used 23m:48s left 22m:37s |===>\n",
            "<===| Training on epoch:1/30 batch:122/236... | time used 24m:0s left 22m:25s |===>\n",
            "<===| Training on epoch:1/30 batch:123/236... | time used 24m:12s left 22m:14s |===>\n",
            "<===| Training on epoch:1/30 batch:124/236... | time used 24m:23s left 22m:1s |===>\n",
            "<===| Training on epoch:1/30 batch:125/236... | time used 24m:35s left 21m:49s |===>\n",
            "<===| Training on epoch:1/30 batch:126/236... | time used 24m:47s left 21m:38s |===>\n",
            "<===| Training on epoch:1/30 batch:127/236... | time used 24m:59s left 21m:26s |===>\n",
            "<===| Training on epoch:1/30 batch:128/236... | time used 25m:11s left 21m:14s |===>\n",
            "<===| Training on epoch:1/30 batch:129/236... | time used 25m:22s left 21m:3s |===>\n",
            "<===| Training on epoch:1/30 batch:130/236... | time used 25m:34s left 20m:51s |===>\n",
            "<===| Training on epoch:1/30 batch:131/236... | time used 25m:46s left 20m:39s |===>\n",
            "<===| Training on epoch:1/30 batch:132/236... | time used 25m:58s left 20m:28s |===>\n",
            "<===| Training on epoch:1/30 batch:133/236... | time used 26m:10s left 20m:16s |===>\n",
            "<===| Training on epoch:1/30 batch:134/236... | time used 26m:22s left 20m:4s |===>\n",
            "<===| Training on epoch:1/30 batch:135/236... | time used 26m:34s left 19m:53s |===>\n",
            "<===| Training on epoch:1/30 batch:136/236... | time used 26m:46s left 19m:41s |===>\n",
            "<===| Training on epoch:1/30 batch:137/236... | time used 26m:58s left 19m:29s |===>\n",
            "<===| Training on epoch:1/30 batch:138/236... | time used 27m:10s left 19m:18s |===>\n",
            "<===| Training on epoch:1/30 batch:139/236... | time used 27m:21s left 19m:5s |===>\n",
            "<===| Training on epoch:1/30 batch:140/236... | time used 27m:33s left 18m:53s |===>\n",
            "<===| Training on epoch:1/30 batch:141/236... | time used 27m:45s left 18m:42s |===>\n",
            "<===| Training on epoch:1/30 batch:142/236... | time used 27m:57s left 18m:30s |===>\n",
            "<===| Training on epoch:1/30 batch:143/236... | time used 28m:9s left 18m:18s |===>\n",
            "<===| Training on epoch:1/30 batch:144/236... | time used 28m:21s left 18m:7s |===>\n",
            "<===| Training on epoch:1/30 batch:145/236... | time used 28m:33s left 17m:55s |===>\n",
            "<===| Training on epoch:1/30 batch:146/236... | time used 28m:45s left 17m:43s |===>\n",
            "<===| Training on epoch:1/30 batch:147/236... | time used 28m:57s left 17m:31s |===>\n",
            "<===| Training on epoch:1/30 batch:148/236... | time used 29m:9s left 17m:20s |===>\n",
            "<===| Training on epoch:1/30 batch:149/236... | time used 29m:21s left 17m:8s |===>\n",
            "<===| Training on epoch:1/30 batch:150/236... | time used 29m:33s left 16m:56s |===>\n",
            "<===| Training on epoch:1/30 batch:151/236... | time used 29m:45s left 16m:45s |===>\n",
            "<===| Training on epoch:1/30 batch:152/236... | time used 29m:57s left 16m:33s |===>\n",
            "<===| Training on epoch:1/30 batch:153/236... | time used 30m:9s left 16m:21s |===>\n",
            "<===| Training on epoch:1/30 batch:154/236... | time used 30m:20s left 16m:9s |===>\n",
            "<===| Training on epoch:1/30 batch:155/236... | time used 30m:32s left 15m:57s |===>\n",
            "<===| Training on epoch:1/30 batch:156/236... | time used 30m:44s left 15m:45s |===>\n",
            "<===| Training on epoch:1/30 batch:157/236... | time used 30m:56s left 15m:34s |===>\n",
            "<===| Training on epoch:1/30 batch:158/236... | time used 31m:8s left 15m:22s |===>\n",
            "<===| Training on epoch:1/30 batch:159/236... | time used 31m:20s left 15m:10s |===>\n",
            "<===| Training on epoch:1/30 batch:160/236... | time used 31m:32s left 14m:58s |===>\n",
            "<===| Training on epoch:1/30 batch:161/236... | time used 31m:44s left 14m:46s |===>\n",
            "<===| Training on epoch:1/30 batch:162/236... | time used 31m:56s left 14m:35s |===>\n",
            "<===| Training on epoch:1/30 batch:163/236... | time used 32m:7s left 14m:23s |===>\n",
            "<===| Training on epoch:1/30 batch:164/236... | time used 32m:19s left 14m:11s |===>\n",
            "<===| Training on epoch:1/30 batch:165/236... | time used 32m:31s left 13m:59s |===>\n",
            "<===| Training on epoch:1/30 batch:166/236... | time used 32m:43s left 13m:48s |===>\n",
            "<===| Training on epoch:1/30 batch:167/236... | time used 32m:55s left 13m:36s |===>\n",
            "<===| Training on epoch:1/30 batch:168/236... | time used 33m:7s left 13m:24s |===>\n",
            "<===| Training on epoch:1/30 batch:169/236... | time used 33m:19s left 13m:12s |===>\n",
            "<===| Training on epoch:1/30 batch:170/236... | time used 33m:31s left 13m:1s |===>\n",
            "<===| Training on epoch:1/30 batch:171/236... | time used 33m:43s left 12m:49s |===>\n",
            "<===| Training on epoch:1/30 batch:172/236... | time used 33m:55s left 12m:37s |===>\n",
            "<===| Training on epoch:1/30 batch:173/236... | time used 34m:7s left 12m:25s |===>\n",
            "<===| Training on epoch:1/30 batch:174/236... | time used 34m:19s left 12m:13s |===>\n",
            "<===| Training on epoch:1/30 batch:175/236... | time used 34m:31s left 12m:2s |===>\n",
            "<===| Training on epoch:1/30 batch:176/236... | time used 34m:43s left 11m:50s |===>\n",
            "<===| Training on epoch:1/30 batch:177/236... | time used 34m:55s left 11m:38s |===>\n",
            "<===| Training on epoch:1/30 batch:178/236... | time used 35m:7s left 11m:26s |===>\n",
            "<===| Training on epoch:1/30 batch:179/236... | time used 35m:19s left 11m:14s |===>\n",
            "<===| Training on epoch:1/30 batch:180/236... | time used 35m:31s left 11m:3s |===>\n",
            "<===| Training on epoch:1/30 batch:181/236... | time used 35m:43s left 10m:51s |===>\n",
            "<===| Training on epoch:1/30 batch:182/236... | time used 35m:55s left 10m:39s |===>\n",
            "<===| Training on epoch:1/30 batch:183/236... | time used 36m:7s left 10m:27s |===>\n",
            "<===| Training on epoch:1/30 batch:184/236... | time used 36m:19s left 10m:16s |===>\n",
            "<===| Training on epoch:1/30 batch:185/236... | time used 36m:31s left 10m:4s |===>\n",
            "<===| Training on epoch:1/30 batch:186/236... | time used 36m:43s left 9m:52s |===>\n",
            "<===| Training on epoch:1/30 batch:187/236... | time used 36m:55s left 9m:40s |===>\n",
            "<===| Training on epoch:1/30 batch:188/236... | time used 37m:6s left 9m:28s |===>\n",
            "<===| Training on epoch:1/30 batch:189/236... | time used 37m:18s left 9m:16s |===>\n",
            "<===| Training on epoch:1/30 batch:190/236... | time used 37m:30s left 9m:4s |===>\n",
            "<===| Training on epoch:1/30 batch:191/236... | time used 37m:42s left 8m:53s |===>\n",
            "<===| Training on epoch:1/30 batch:192/236... | time used 37m:54s left 8m:41s |===>\n",
            "<===| Training on epoch:1/30 batch:193/236... | time used 38m:6s left 8m:29s |===>\n",
            "<===| Training on epoch:1/30 batch:194/236... | time used 38m:18s left 8m:17s |===>\n",
            "<===| Training on epoch:1/30 batch:195/236... | time used 38m:30s left 8m:5s |===>\n",
            "<===| Training on epoch:1/30 batch:196/236... | time used 38m:42s left 7m:53s |===>\n",
            "<===| Training on epoch:1/30 batch:197/236... | time used 38m:54s left 7m:42s |===>\n",
            "<===| Training on epoch:1/30 batch:198/236... | time used 39m:6s left 7m:30s |===>\n",
            "<===| Training on epoch:1/30 batch:199/236... | time used 39m:18s left 7m:18s |===>\n",
            "<===| Training on epoch:1/30 batch:200/236... | time used 39m:30s left 7m:6s |===>\n",
            "<===| Training on epoch:1/30 batch:201/236... | time used 39m:42s left 6m:54s |===>\n",
            "<===| Training on epoch:1/30 batch:202/236... | time used 39m:54s left 6m:43s |===>\n",
            "<===| Training on epoch:1/30 batch:203/236... | time used 40m:6s left 6m:31s |===>\n",
            "<===| Training on epoch:1/30 batch:204/236... | time used 40m:18s left 6m:19s |===>\n",
            "<===| Training on epoch:1/30 batch:205/236... | time used 40m:30s left 6m:7s |===>\n",
            "<===| Training on epoch:1/30 batch:206/236... | time used 40m:42s left 5m:55s |===>\n",
            "<===| Training on epoch:1/30 batch:207/236... | time used 40m:54s left 5m:43s |===>\n",
            "<===| Training on epoch:1/30 batch:208/236... | time used 41m:6s left 5m:31s |===>\n",
            "<===| Training on epoch:1/30 batch:209/236... | time used 41m:18s left 5m:20s |===>\n",
            "<===| Training on epoch:1/30 batch:210/236... | time used 41m:30s left 5m:8s |===>\n",
            "<===| Training on epoch:1/30 batch:211/236... | time used 41m:41s left 4m:56s |===>\n",
            "<===| Training on epoch:1/30 batch:212/236... | time used 41m:53s left 4m:44s |===>\n",
            "<===| Training on epoch:1/30 batch:213/236... | time used 42m:5s left 4m:32s |===>\n",
            "<===| Training on epoch:1/30 batch:214/236... | time used 42m:17s left 4m:20s |===>\n",
            "<===| Training on epoch:1/30 batch:215/236... | time used 42m:29s left 4m:8s |===>\n",
            "<===| Training on epoch:1/30 batch:216/236... | time used 42m:41s left 3m:57s |===>\n",
            "<===| Training on epoch:1/30 batch:217/236... | time used 42m:53s left 3m:45s |===>\n",
            "<===| Training on epoch:1/30 batch:218/236... | time used 43m:4s left 3m:33s |===>\n",
            "<===| Training on epoch:1/30 batch:219/236... | time used 43m:16s left 3m:21s |===>\n",
            "<===| Training on epoch:1/30 batch:220/236... | time used 43m:28s left 3m:9s |===>\n",
            "<===| Training on epoch:1/30 batch:221/236... | time used 43m:40s left 2m:57s |===>\n",
            "<===| Training on epoch:1/30 batch:222/236... | time used 43m:52s left 2m:46s |===>\n",
            "<===| Training on epoch:1/30 batch:223/236... | time used 44m:4s left 2m:34s |===>\n",
            "<===| Training on epoch:1/30 batch:224/236... | time used 44m:15s left 2m:22s |===>\n",
            "<===| Training on epoch:1/30 batch:225/236... | time used 44m:27s left 2m:10s |===>\n",
            "<===| Training on epoch:1/30 batch:226/236... | time used 44m:39s left 1m:58s |===>\n",
            "<===| Training on epoch:1/30 batch:227/236... | time used 44m:51s left 1m:46s |===>\n",
            "<===| Training on epoch:1/30 batch:228/236... | time used 45m:3s left 1m:34s |===>\n",
            "<===| Training on epoch:1/30 batch:229/236... | time used 45m:14s left 1m:22s |===>\n",
            "<===| Training on epoch:1/30 batch:230/236... | time used 45m:26s left 1m:11s |===>\n",
            "<===| Training on epoch:1/30 batch:231/236... | time used 45m:38s left 0m:59s |===>\n",
            "<===| Training on epoch:1/30 batch:232/236... | time used 45m:49s left 0m:47s |===>\n",
            "<===| Training on epoch:1/30 batch:233/236... | time used 46m:1s left 0m:35s |===>\n",
            "<===| Training on epoch:1/30 batch:234/236... | time used 46m:12s left 0m:23s |===>\n",
            "<===| Training on epoch:1/30 batch:235/236... | time used 46m:24s left 0m:11s |===>\n",
            "<===| Training on epoch:1/30 batch:236/236... | time used 46m:36s left 0m:0s |===>\n",
            "<===| Testing on validation set with 7085 sentence pairs, please be patient! |===>\n",
            "\n",
            "tcmalloc: large alloc 6699696128 bytes == 0x61d10000 @  0x7fb55fea8b6b 0x7fb55fec8379 0x7fb50d681e4e 0x7fb50d683dca 0x7fb50e2c7d62 0x7fb50e3e5b1b 0x7fb50e2c5a40 0x7fb50e568319 0x7fb555de8e4a 0x4f8925 0x4f98c7 0x4f6128 0x4f7d60 0x4f876d 0x4f98c7 0x4f6128 0x4f7d60 0x4f876d 0x4f98c7 0x4f6128 0x4f9023 0x6415b2 0x64166a 0x643730 0x62b26e 0x4b4cb0 0x7fb55fac3b97 0x5bdf6a\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 79, in <module>\n",
            "    clip, encoder_learning_rate, decoder_learning_rate, test_size, random_state)\n",
            "  File \"/content/Text_Generation_with_Attention_Mechanism/src/attention/iteration.py\", line 199, in trainIter\n",
            "    loss_test += validation(input_batches, input_lengths, target_batches, target_lengths, encoder, decoder)\n",
            "  File \"/content/Text_Generation_with_Attention_Mechanism/src/attention/iteration.py\", line 152, in validation\n",
            "    all_decoder_outputs.transpose(0, 1).contiguous(),\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 6.24 GiB (GPU 0; 11.17 GiB total capacity; 6.74 GiB already allocated; 4.02 GiB free; 90.36 MiB cached)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KlYn9bd0hCA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "df046cf5-dd98-4623-db0a-c6009f83991d"
      },
      "source": [
        "!python evaluate_drake.py"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.GRU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "<=== Seed_text: jumpman jumpman jumpman ===>\n",
            "<=== Generated_lyrics: i m a done i t t be a place <EOS> ===>\n",
            "<Figure size 1000x800 with 2 Axes>\n",
            "<=== Seed_text: i guess you lose some and win some ===>\n",
            "<=== Generated_lyrics: i m just to the friends and you know you re <EOS> ===>\n",
            "<Figure size 1000x800 with 2 Axes>\n",
            "<=== Seed_text: people drain me energy ===>\n",
            "<=== Generated_lyrics: i m a good to you <EOS> ===>\n",
            "<Figure size 1000x800 with 2 Axes>\n",
            "<=== Seed_text: them boys up to something ===>\n",
            "<=== Generated_lyrics: i m just to the cut and you know it <EOS> ===>\n",
            "<Figure size 1000x800 with 2 Axes>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5ZsqZteyb7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/drive/My\\ Drive/Zairja/data/embeddings/glove.6B.50d.txt  ../data/glove.6B.50d.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6s3punAxpzh",
        "colab_type": "code",
        "outputId": "f40b1266-8168-4078-f2d4-7d5ce23836cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "script=open('train.py').read()\n",
        "script=script.replace()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import os\\nimport torch\\nimport numpy as np\\nfrom attention.helper import del_save\\nfrom attention.iteration import trainIter\\nfrom attention.preprocessing import prep_data\\nfrom attention.model import EncoderRNN, LuongAttnDecoderRNN\\n\\nUSE_CUDA = True\\n\\n# feel free to delete pretrained model and cleaned dataset\\n#######################################################################################################\\n# remove embeddings\\ndel_save(\\'save/vocab.pt\\')\\n# remove cleaned dataset\\ndel_save(\\'save/pairs.npy\\')\\n# remove pretrained models\\ndel_save(\\'save/encoder.pt\\')\\ndel_save(\\'save/decoder.pt\\')\\n# remove loss_array\\ndel_save(\\'save/loss.npy\\')\\n\\n\\n#######################################################################################################\\n# Data preparation, hyperparameters are embedding_size, min/max_length of sequences\\n\\nhidden_size = 50 # embedding_dim, modify preprocessing.embeds accordingly if you want to change this\\n# data preparation\\ndata_dir = \\'../data/pop\\'\\nMAX_LENGTH = 15 # max length to trim\\n# load/instantiate lang and pairs\\nif not ( os.path.exists(\\'save/vocab.pt\\') and os.path.exists(\\'save/pairs.npy\\') ):\\n\\tlang, pairs = prep_data(data_dir, MAX_LENGTH)\\n\\ttorch.save(lang, \\'save/vocab.pt\\')\\n\\tnp.save(\\'save/pairs.npy\\', pairs)\\nelse:\\n\\tlang = torch.load(\\'save/vocab.pt\\')\\n\\tpairs = np.load(\\'save/pairs.npy\\')\\nprint(\"<====================| Dataset Summary |====================>\")\\nprint(\"<===| Loaded {:d} sentence pairs | {:d} unique words |===>\\\\n\".format(len(pairs), lang.n_words))\\n\\n#######################################################################################################\\n# Instantiate encoder/decoder, hyperparameter: dropout, n_layers, attention type, optimizers(given in trainIter)\\n\\nn_layers = 2 # number of gru layers\\ndropout=0.1 # dropout rate\\nattention_model = \\'general\\' # attention type\\n# instantiate or load encoder/decoder\\nif not os.path.exists(\\'save/encoder.pt\\'):\\n\\tencoder = EncoderRNN(lang, hidden_size, n_layers)\\nelse: encoder = torch.load(\\'save/encoder.pt\\')\\nif not os.path.exists(\\'save/decoder.pt\\'):\\n\\tdecoder = LuongAttnDecoderRNN(attention_model, lang, hidden_size, n_layers, dropout)\\nelse: decoder = torch.load(\\'save/decoder.pt\\')\\n# enable train mode\\nencoder.train()\\ndecoder.train()\\n# Use GPU\\nif USE_CUDA:\\n\\t    encoder = encoder.cuda()\\n\\t    decoder = decoder.cuda()\\n\\n######################################################################################################\\n# Iteration, corresponding hyperparameters are given below\\n\\nbatch_size = 100\\nn_epochs = 30\\nclip = 5.0\\nencoder_learning_rate = 0.001\\ndecoder_learning_rate = 0.003\\ntest_size = 0.3 # train/validation ratio\\nrandom_state = 666 # random_state of train/test split\\n# let the fun begin\\nprint(\\'<====================| Training Summary |====================>\\')\\nprint(\\'<===| Epochs:{:d} | BatchSize:{:d} | ClipValue:{:.1f} | EncoderLR:{:.1e} | DecoderLR:{:.1e} | ValidationSize:{:.2f} |===>\\\\n\\'\\n\\t.format(n_epochs, batch_size, clip, encoder_learning_rate, decoder_learning_rate, test_size))\\n# train model\\ntrainIter(n_epochs, batch_size, lang, pairs, encoder, decoder,\\n\\tclip, encoder_learning_rate, decoder_learning_rate, test_size, random_state)\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf9Obt8exSU1",
        "colab_type": "code",
        "outputId": "e9294a3f-13d9-44a2-f088-a340ffd7a905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "x=open('../data/glove.6B.50d.txt').read()\n",
        "x[:50]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-b94f17b88b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/glove.6B.50d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/glove.6B.50d.txt'"
          ]
        }
      ]
    }
  ]
}